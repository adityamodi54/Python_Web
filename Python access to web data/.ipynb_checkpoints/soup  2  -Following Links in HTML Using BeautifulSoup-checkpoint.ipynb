{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.py4e.com/tools/python-data/?PHPSESSID=eb5a050b886d78872cc323ffda9351c5\n",
    "# https://www.py4e.com/tools/python-data/?PHPSESSID=8cc8f017f7421c8b6691f96c94920376\n",
    "\n",
    "\n",
    "# http://py4e-data.dr-chuck.net/known_by_Kirk.html\n",
    "\n",
    "\n",
    "# In this assignment you will write a Python program that expands on http://www.py4e.com/code3/urllinks.py. \n",
    "# The program will use urllib to read the HTML from the data files below, \n",
    "\n",
    "\n",
    "# # To run this, you can install BeautifulSoup\n",
    "# # https://pypi.python.org/pypi/beautifulsoup4\n",
    "\n",
    "# # Or download the file\n",
    "# # http://www.py4e.com/code3/bs4.zip\n",
    "# # and unzip it in the same directory as this file\n",
    "\n",
    "# import urllib.request, urllib.parse, urllib.error\n",
    "# from bs4 import BeautifulSoup\n",
    "# import ssl\n",
    "\n",
    "# # Ignore SSL certificate errors\n",
    "# ctx = ssl.create_default_context()\n",
    "# ctx.check_hostname = False\n",
    "# ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "# url = input('Enter - ')\n",
    "# html = urllib.request.urlopen(url, context=ctx).read()\n",
    "# soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# # Retrieve all of the anchor tags\n",
    "# tags = soup('a')\n",
    "# for tag in tags:\n",
    "#     print(tag.get('href', None))\n",
    "\n",
    "\n",
    "# extract the href= vaues from the anchor tags, \n",
    "# scan for a tag that is in a particular position relative to the first name in the list, \n",
    "# follow that link and repeat the process a number of times and report the last name you find.\n",
    "\n",
    "\n",
    "# Sample problem: \n",
    "# Start at http://py4e-data.dr-chuck.net/known_by_Fikret.html \n",
    "# Find the link at position 3 (the first name is 1). \n",
    "# Follow that link. \n",
    "# Repeat this process 4 times. \n",
    "# The answer is the last name that you retrieve.\n",
    "\n",
    "# Sequence of names: Fikret Montgomery Mhairade Butchi Anayah \n",
    "# Last name in sequence: Anayah\n",
    "\n",
    "# Actual problem: Start at: http://py4e-data.dr-chuck.net/known_by_Kirk.html \n",
    "# Find the link at position 18 (the first name is 1). \n",
    "# Follow that link. \n",
    "# Repeat this process 7 times. \n",
    "# The answer is the last name that you retrieve.\n",
    "# Hint: The first character of the name of the last page that you will load is: K\n",
    "\n",
    "# Strategy\n",
    "# The web pages tweak the height between the links \n",
    "# and hide the page after a few seconds to make it difficult for you to do the assignment without writing a Python program. \n",
    "# But frankly with a little effort and patience \n",
    "# you can overcome these attempts to make it a little harder to complete the assignment without writing a Python program. \n",
    "# But that is not the point. \n",
    "# The point is to write a clever Python program to solve the program.\n",
    "\n",
    "# Sample execution\n",
    "\n",
    "# Here is a sample execution of a solution:\n",
    "\n",
    "# $ python3 solution.py\n",
    "# Enter URL: http://py4e-data.dr-chuck.net/known_by_Fikret.html\n",
    "# Enter count: 4\n",
    "# Enter position: 3\n",
    "# Retrieving: http://py4e-data.dr-chuck.net/known_by_Fikret.html\n",
    "# Retrieving: http://py4e-data.dr-chuck.net/known_by_Montgomery.html\n",
    "# Retrieving: http://py4e-data.dr-chuck.net/known_by_Mhairade.html\n",
    "# Retrieving: http://py4e-data.dr-chuck.net/known_by_Butchi.html\n",
    "# Retrieving: http://py4e-data.dr-chuck.net/known_by_Anayah.html\n",
    "# The answer to the assignment for this execution is \"Anayah\".\n",
    "# Turning in the Assignment\n",
    "\n",
    "# Enter the last name retrieved and your Python code below:\n",
    "# Name: \n",
    "#  (name starts with K) \n",
    "# Python code:\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nSample data: http://py4e-data.dr-chuck.net/comments_42.html (Sum=2553)\\nActual data: http://py4e-data.dr-chuck.net/comments_113065.html (Sum ends with 17)\\n\\n        code\\n# To run this, you can install BeautifulSoup\\n# https://pypi.python.org/pypi/beautifulsoup4\\n\\n# Or download the file\\n# http://www.py4e.com/code3/bs4.zip\\n# and unzip it in the same directory as this file\\n\\nfrom urllib.request import urlopen\\nfrom bs4 import BeautifulSoup\\nimport ssl\\n\\n# Ignore SSL certificate errors\\nctx = ssl.create_default_context()\\nctx.check_hostname = False\\nctx.verify_mode = ssl.CERT_NONE\\n\\nurl = input(\\'Enter - \\')\\nhtml = urlopen(url, context=ctx).read()\\nsoup = BeautifulSoup(html, \"html.parser\")\\n\\n# Retrieve all of the anchor tags\\ntags = soup(\\'a\\')\\nfor tag in tags:\\n    # Look at the parts of a tag\\n    print(\\'TAG:\\', tag)\\n    print(\\'URL:\\', tag.get(\\'href\\', None))\\n    print(\\'Contents:\\', tag.contents[0])\\n    print(\\'Attrs:\\', tag.attrs)\\n    \\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "Sample data: http://py4e-data.dr-chuck.net/comments_42.html (Sum=2553)\n",
    "Actual data: http://py4e-data.dr-chuck.net/comments_113065.html (Sum ends with 17)\n",
    "\n",
    "        code\n",
    "# To run this, you can install BeautifulSoup\n",
    "# https://pypi.python.org/pypi/beautifulsoup4\n",
    "\n",
    "# Or download the file\n",
    "# http://www.py4e.com/code3/bs4.zip\n",
    "# and unzip it in the same directory as this file\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "url = input('Enter - ')\n",
    "html = urlopen(url, context=ctx).read()\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# Retrieve all of the anchor tags\n",
    "tags = soup('a')\n",
    "for tag in tags:\n",
    "    # Look at the parts of a tag\n",
    "    print('TAG:', tag)\n",
    "    print('URL:', tag.get('href', None))\n",
    "    print('Contents:', tag.contents[0])\n",
    "    print('Attrs:', tag.attrs)\n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nHow to click with bs4\\nand save the name\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "Get all links on page\n",
    "How to click on lin ks with bs4 , based on position\n",
    "retrive link\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "read links\n",
    "appent in list\n",
    "list position number\n",
    "new link\n",
    "for loops run again\n",
    "final link = answer\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run this, you can install BeautifulSoup\n",
    "# https://pypi.python.org/pypi/beautifulsoup4\n",
    "\n",
    "# Or download the file\n",
    "# http://www.py4e.com/code3/bs4.zip\n",
    "# and unzip it in the same directory as this file\n",
    "\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "url = input('Enter - ')\n",
    "html = urllib.request.urlopen(url, context=ctx).read()\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Retrieve all of the anchor tags\n",
    "tags = soup('a')\n",
    "for tag in tags:\n",
    "    print(tag.get('href', None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://py4e-data.dr-chuck.net/known_by_Kirk.htmlhttp://py4e-data.dr-chuck.net/known_by_Kirk.html\n",
      "Enter count: 7\n",
      "Enter position: 18\n",
      "Retrieving:  http://py4e-data.dr-chuck.net/known_by_Kirk.html\n",
      "Retrieving:  http://py4e-data.dr-chuck.net/known_by_Fenn.html\n",
      "Retrieving:  http://py4e-data.dr-chuck.net/known_by_Francis.html\n",
      "Retrieving:  http://py4e-data.dr-chuck.net/known_by_Keyra.html\n",
      "Retrieving:  http://py4e-data.dr-chuck.net/known_by_Meko.html\n",
      "Retrieving:  http://py4e-data.dr-chuck.net/known_by_Darla.html\n",
      "Retrieving:  http://py4e-data.dr-chuck.net/known_by_Cain.html\n",
      "Last Url:  http://py4e-data.dr-chuck.net/known_by_Karyss.html\n"
     ]
    }
   ],
   "source": [
    "import urllib.request as ur\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "current_repeat_count = 0\n",
    "url = input('http://py4e-data.dr-chuck.net/known_by_Kirk.html')\n",
    "repeat_count = int(input('Enter count: '))\n",
    "position = int(input('Enter position: '))\n",
    "\n",
    "\n",
    "def parse_html(url):\n",
    "    html = ur.urlopen(url).read()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    tags = soup('a')\n",
    "    return tags\n",
    "\n",
    "while current_repeat_count < repeat_count:                            # ???????\n",
    "    print('Retrieving: ', url)\n",
    "    tags = parse_html(url)\n",
    "    \n",
    "    for index, item in enumerate(tags):                              # ????????????\n",
    "        if index == position - 1:\n",
    "            url = item.get('href', None)\n",
    "            name = item.contents[0]\n",
    "            break\n",
    "        else:\n",
    "            continue\n",
    "    current_repeat_count += 1\n",
    "print('Last Url: ', url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kirk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aditya Modi\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\Users\\Aditya Modi\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fenn\n",
      "Francis\n",
      "Keyra\n",
      "Meko\n",
      "Darla\n",
      "Cain\n",
      "Karyss\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "import re\n",
    "\n",
    "scontext = ssl.SSLContext(ssl.PROTOCOL_TLSv1)\n",
    "depth = 8\n",
    "initial_url = 'http://py4e-data.dr-chuck.net/known_by_Kirk.html'\n",
    "\n",
    "def follow_url(url, depth):\n",
    "    if depth > 0:\n",
    "        get_name(url)\n",
    "        soup = BeautifulSoup(urllib.request.urlopen(url, context=scontext).read())\n",
    "        follow_url(soup('a')[17].get('href', None), depth - 1)\n",
    "\n",
    "def get_name(url):\n",
    "    print (re.findall('known_by_([A-Z][a-z]+).html', url)[0],)\n",
    "\n",
    "follow_url(initial_url, depth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter count: 7\n",
      "Enter position: 18\n",
      "Retrieving: http://py4e-data.dr-chuck.net/known_by_Kirk.html\n",
      "Retrieving: http://py4e-data.dr-chuck.net/known_by_Fenn.html\n",
      "Retrieving: http://py4e-data.dr-chuck.net/known_by_Francis.html\n",
      "Retrieving: http://py4e-data.dr-chuck.net/known_by_Keyra.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aditya Modi\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\Users\\Aditya Modi\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving: http://py4e-data.dr-chuck.net/known_by_Meko.html\n",
      "Retrieving: http://py4e-data.dr-chuck.net/known_by_Darla.html\n",
      "Retrieving: http://py4e-data.dr-chuck.net/known_by_Cain.html\n",
      "Retrieving: http://py4e-data.dr-chuck.net/known_by_Karyss.html\n",
      "Last name found: Karyss\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = ('http://py4e-data.dr-chuck.net/known_by_Kirk.html')\n",
    "count = int(input('Enter count: '))\n",
    "position = int(input('Enter position: '))\n",
    "\n",
    "while count >= 0:\n",
    "    name = re.findall('known_by_(.+).html', url)\n",
    "    print ('Retrieving: ' + url)\n",
    "    html = urllib.request.urlopen(url).read()\n",
    "    soup = BeautifulSoup(html)\n",
    "    # Retrieve all the a tags:\n",
    "    links = soup('a')\n",
    "    url = links[position - 1].get('href', None)\n",
    "    count = count - 1\n",
    "\n",
    "print ('Last name found: ' + name[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter count: 7\n",
      "Enter position: 18\n",
      "retrieving: http://py4e-data.dr-chuck.net/known_by_Kirk.html\n",
      "retrieving: http://py4e-data.dr-chuck.net/known_by_Fenn.html\n",
      "retrieving: http://py4e-data.dr-chuck.net/known_by_Francis.html\n",
      "retrieving: http://py4e-data.dr-chuck.net/known_by_Keyra.html\n",
      "retrieving: http://py4e-data.dr-chuck.net/known_by_Meko.html\n",
      "retrieving: http://py4e-data.dr-chuck.net/known_by_Darla.html\n",
      "retrieving: http://py4e-data.dr-chuck.net/known_by_Cain.html\n",
      "Karyss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aditya Modi\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\Users\\Aditya Modi\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = (\"http://py4e-data.dr-chuck.net/known_by_Kirk.html\")\n",
    "count = int(input(\"Enter count: \"))\n",
    "position = int(input(\"Enter position: \"))\n",
    "\n",
    "names = []\n",
    "\n",
    "while count > 0:\n",
    "    print (\"retrieving: {0}\".format(url))\n",
    "    page = urllib.request.urlopen(url)\n",
    "    soup = BeautifulSoup(page)\n",
    "    anchors = soup('a')\n",
    "    name = anchors[position-1].string\n",
    "    names.append(name)\n",
    "    url = anchors[position-1]['href']\n",
    "    count -= 1\n",
    "\n",
    "print (names[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter count: 7\n",
      "7\n",
      "Enter position: 18\n",
      "18\n",
      "Retrieving:  http://py4e-data.dr-chuck.net/known_by_Kirk.html\n",
      "Retrieving:  http://py4e-data.dr-chuck.net/known_by_Fenn.html\n",
      "Retrieving:  http://py4e-data.dr-chuck.net/known_by_Francis.html\n",
      "Retrieving:  http://py4e-data.dr-chuck.net/known_by_Keyra.html\n",
      "Retrieving:  http://py4e-data.dr-chuck.net/known_by_Meko.html\n",
      "Retrieving:  http://py4e-data.dr-chuck.net/known_by_Darla.html\n",
      "Retrieving:  http://py4e-data.dr-chuck.net/known_by_Cain.html\n",
      "Karyss\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = (\"http://py4e-data.dr-chuck.net/known_by_Kirk.html\")\n",
    "\n",
    "count = int(input(\"Enter count: \"))\n",
    "print (count)\n",
    "position = int(input(\"Enter position: \"))\n",
    "print (position)\n",
    "\n",
    "while True:\n",
    "    # print count\n",
    "    print (\"Retrieving: \", url)\n",
    "    html = urllib.request.urlopen(url).read()\n",
    "    soup = BeautifulSoup(html,\"lxml\")    \n",
    "    atags = soup('a')\n",
    "    \n",
    "    # print atags[position].contents[0]\n",
    "    url = atags[position-1].get('href',None)\n",
    "    # print url\n",
    "\n",
    "\n",
    "    count -= 1\n",
    "    if count == -0:\n",
    "        # print atags\n",
    "        print (atags[position-1].contents[0])\n",
    "        break\n",
    "    atags = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter - http://py4e-data.dr-chuck.net/known_by_Kirk.html\n",
      "Retrieving: http://py4e-data.dr-chuck.net/known_by_Kirk.html\n",
      "Retrieving: http://py4e-data.dr-chuck.net/known_by_Fenn.html\n",
      "Retrieving: http://py4e-data.dr-chuck.net/known_by_Francis.html\n",
      "Retrieving: http://py4e-data.dr-chuck.net/known_by_Keyra.html\n",
      "Retrieving: http://py4e-data.dr-chuck.net/known_by_Meko.html\n",
      "Retrieving: http://py4e-data.dr-chuck.net/known_by_Darla.html\n",
      "Retrieving: http://py4e-data.dr-chuck.net/known_by_Cain.html\n",
      "Retrieving: http://py4e-data.dr-chuck.net/known_by_Karyss.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aditya Modi\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\Users\\Aditya Modi\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url=input('Enter - ')\n",
    "i=0\n",
    "while i<8:\n",
    "\thtml=urllib.request.urlopen(url).read()\n",
    "\tprint ('Retrieving:',url)\n",
    "\tsoup=BeautifulSoup(html)\n",
    "\ttags=soup('a')\n",
    "\turl=tags[17].get('href',None)\n",
    "\ti+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
